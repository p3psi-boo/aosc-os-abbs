From fe863761a39f62eb9ec2c9a83e39779c0e7d404e Mon Sep 17 00:00:00 2001
From: Chao Li <lichao@loongson.cn>
Date: Tue, 11 Jun 2024 19:37:11 +0800
Subject: [PATCH 08/43] system/cache: Add LoongArch64 cache operations support

Added cache operations support for LoongArch64.

Signed-off-by: Chao Li <lichao@loongson.cn>
---
 system/cache.h | 60 ++++++++++++++++++++++++++++++++++++++++++++++----
 1 file changed, 56 insertions(+), 4 deletions(-)

diff --git a/system/cache.h b/system/cache.h
index 3a40550..0d89f55 100644
--- a/system/cache.h
+++ b/system/cache.h
@@ -10,12 +10,23 @@
  * Copyright (C) 2020-2022 Martin Whitaker.
  */
 
+#ifdef __loongarch_lp64
+#include <larchintrin.h>
+#include "string.h"
+#define cache_op(op,addr)     \
+    __asm__ __volatile__(     \
+    "cacop %0, %1\n"          \
+    :                         \
+    : "i" (op), "ZC" (*(unsigned char *)(addr)))
+static inline void cache_flush(void);
+#endif
+
 /**
  * Disable the CPU caches.
  */
 static inline void cache_off(void)
 {
-#ifdef __x86_64__
+#if defined(__x86_64__)
     __asm__ __volatile__ ("\t"
         "movq   %%cr0, %%rax        \n\t"
         "orl    $0x40000000, %%eax  \n\t"  /* Set CD */
@@ -25,7 +36,7 @@ static inline void cache_off(void)
         : /* no inputs */
         : "rax", "memory"
     );
-#else
+#elif defined(__i386__)
     __asm__ __volatile__ ("\t"
         "movl   %%cr0, %%eax        \n\t"
         "orl    $0x40000000, %%eax  \n\t"  /* Set CD */
@@ -35,6 +46,9 @@ static inline void cache_off(void)
         : /* no inputs */
         : "eax", "memory"
     );
+#elif defined(__loongarch_lp64)
+    cache_flush();
+    __csrxchg_d(0, 3 << 4, 0x181);
 #endif
 }
 
@@ -43,7 +57,7 @@ static inline void cache_off(void)
  */
 static inline void cache_on(void)
 {
-#ifdef __x86_64__
+#if defined(__x86_64__)
     __asm__ __volatile__ ("\t"
         "movq   %%cr0, %%rax        \n\t"
         "andl   $0x9fffffff, %%eax  \n\t" /* Clear CD and NW */
@@ -52,7 +66,7 @@ static inline void cache_on(void)
         : /* no inputs */
         : "rax", "memory"
     );
-#else
+#elif defined(__i386__)
     __asm__ __volatile__ ("\t"
         "movl   %%cr0, %%eax        \n\t"
         "andl   $0x9fffffff, %%eax  \n\t" /* Clear CD and NW */
@@ -61,6 +75,9 @@ static inline void cache_on(void)
         : /* no inputs */
         : "eax", "memory"
     );
+#elif defined(__loongarch_lp64)
+    cache_flush();
+    __csrxchg_d(1 << 4, 3 << 4, 0x181);
 #endif
 }
 
@@ -69,12 +86,47 @@ static inline void cache_on(void)
  */
 static inline void cache_flush(void)
 {
+#if defined(__i386__) || defined(__x86_64__)
     __asm__ __volatile__ ("\t"
         "wbinvd\n"
         : /* no outputs */
         : /* no inputs */
         : "memory"
     );
+#elif defined (__loongarch_lp64)
+    if (!(__cpucfg(0x10) & (1 << 10))) {
+        return; // No L3
+    }
+    uint64_t ways = (__cpucfg(0x14) & 0xFFFF) + 1;
+    uint64_t sets = 1 << ((__cpucfg(0x14) >> 16) & 0xFF);
+    uint64_t line_size = 1 << ((__cpucfg(0x14) >> 24) & 0x7F);
+    uint64_t va, i, j;
+    uint64_t cpu_module[1];
+    va = 0;
+
+    cpu_module[0] = (uint64_t)__iocsrrd_d(0x20);
+    if (strstr((const char *)cpu_module, "A6")) {
+        uint8_t old_sc_cfg;
+        old_sc_cfg = __iocsrrd_b(0x280);
+        __iocsrwr_b(0x1, 0x280);
+        for (i = 0; i < (ways * 3); i++) {
+            for (j = 0; j < sets; j++) {
+                *(volatile uint32_t *)va;
+                va += line_size;
+            }
+        }
+        __iocsrwr_b(old_sc_cfg, 0x280);
+    } else {
+        for (i = 0; i < sets; i++) {
+            for (j = 0; j < ways; j++) {
+                cache_op(0xB, va);
+                va++;
+            }
+            va -= ways;
+            va += line_size;
+        }
+    }
+#endif
 }
 
 #endif // CACHE_H
-- 
2.43.4

